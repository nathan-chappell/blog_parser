# FontFace - custom fonts for your sites
- filename: 2012/01/24/FontFace-custom-fonts-for-your-sites/index.html
  article_title: FontFace - custom fonts for your sites
  question: what's the best way to use custom fonts on a website?
  answer: font-face
  context: There are several approaches for integrating custom fonts to standards-compliant web pages - some of them use custom scripts (Cufon, sIFR, FLIR), and others use pure-CSS solutions, like @font-face. All of these technologies have valid arguments both for and against their use, but probably the most flexible and safest, and in general the best method for using various non-system fonts on your web site is @font-face.

- filename: 2012/01/24/FontFace-custom-fonts-for-your-sites/index.html
  article_title: FontFace - custom fonts for your sites
  question: what's the easiest way to get web fonts?
  answer: pick them up from various web sites
  context: The easiest way to get web fonts is to pick them up from various web sites that hold vast font repositories of free or commercial fonts.

# 2012/12/20/Building-Cordova-2-2-0/index.html

- filename: 2012/12/20/Building-Cordova-2-2-0/index.html
  article_title: Building Cordova 2.2.0
  question: what causes the error The import org.apache.cordova cannot be resolved
  answer: Missing cordova-2.2.0.jar file
  context: Missing cordova-2.2.0.jar file is causing the infamous The import org.apache.cordova cannot be resolved error. You should have Eclipse, Android SDK and the ADT plugin already installed on your machine, the PATH environment variables should be set up. Create a new project using

# 2013/01/07/Installing-Redmine-and-Ubuntu-on-VirtualBox/index.html

- filename: 2013/01/07/Installing-Redmine-and-Ubuntu-on-VirtualBox/index.html
  article_title: Installing Redmine and Ubuntu on VirtualBox
  question: After installing virtualbox, what's the first thing you need to do?
  answer: install the VirtualBox guest additions
  context: After restarting the machine for the first time and logging in we need to install the VirtualBox guest additions to get access to all available features of the guest machine (mouse integration, full resolution, etc.). Just press Host+D or choose Install Guest Additions… action from Devices menu.

- filename: 2013/01/07/Installing-Redmine-and-Ubuntu-on-VirtualBox/index.html
  article_title: Installing Redmine and Ubuntu on VirtualBox
  question: How should I install redmine on ubuntu?
  answer: Downloading and building Redmine from source is a preferred way to install it
  context: Now we need to get Redmine source. Downloading and building Redmine from source is a preferred way to install it since we will benefit from the latest fixes and upgrades provided by the community. Subversion is included in Ubuntu and I'll use it to get latest source. You can find instructions for code checkout on the Redmine site Download section - just enter the command below and the last stable release will be checked out to subdolfer redmine-2.1

- filename: 2013/01/07/Installing-Redmine-and-Ubuntu-on-VirtualBox/index.html
  article_title: Installing Redmine and Ubuntu on VirtualBox
  question: What is a Gemfile?
  answer: it contains configuration settings
  context: After downloading the code we need to modify a few configuration files in Redmine project to support our development environment. We will start with Gemfile, as it contains configuration settings for all required gems. I ended up with changing these lines to accommodate our environment

# 2013/02/01/ASP-NET-WebAPI-APNS-and-Android-working-together/index.html

- filename: 2013/02/01/ASP-NET-WebAPI-APNS-and-Android-working-together/index.html
  article_title: ASP.NET WebAPI, APNS and Android working together
  question: What is moon-apns?
  answer: an open source C# library
  context: We will use an open source C# library called Moon-APNS to send messages to APNS. You can find more info on it on arashnorouzi blog. Just download the library from GitHub, add its project to the solution and add references to the Model project and Moon-APNS project from the ServerApp.

# 2013/02/17/Delegates-in-action/index.html

- filename: 2013/02/17/Delegates-in-action/index.html
  article_title: Delegates in action
  question: What are delegates in .NET?
  answer: basically function pointers
  context: The main idea is to declare two delegates. As we all know, delegates in .NET are basically function pointers. The first one is used to reference a repository method, and the second references an action performed on each entity.

# 2013/03/08/SQL-Tips-Tricks-changing-column-values-in-multiple-tables/index.html

# 2013/03/10/Custom-fonts-using-Cufon-on-Windows-Phone/index.html

- filename: 2013/03/10/Custom-fonts-using-Cufon-on-Windows-Phone/index.html
  article_title: Custom fonts using Cufon on Windows Phone
  question: What browser does not support @font-face?
  answer: Internet Explorer Mobile on Windows Phone 7.5
  context: Every major browser platform supports @font-face, except Internet Explorer Mobile on Windows Phone 7.5. This problem can be solved using Cufon, a javascript text-image replacement solution that combines HTML5 canvas and VML to render the fonts.

- filename: 2013/03/10/Custom-fonts-using-Cufon-on-Windows-Phone/index.html
  article_title: Custom fonts using Cufon on Windows Phone
  question: Which css rules are not supported by cufon?
  answer: line-height
  context: "There is one issue that remains to be solved: line-height css rule is not supported by Cufon. You can work around this limitation by"

# 2013/04/22/How-to-prevent-data-loss-during-delete-update-in-SQL-Server/index.html

- filename: 2013/04/22/How-to-prevent-data-loss-during-delete-update-in-SQL-Server/index.html
  article_title: How to prevent data loss during delete update in SQL Server
  question: How can I test an edit/ delete script without backup?
  answer: run it inside of a BEGIN TRANSACTION / ROLLBACK TRANSACTION block
  context: "What if you have a large DB that takes long to back-up and you really don't have that time? What if you absolutely must do the edit/update without backup. There is a way to test of the script is running fine: run it inside of a BEGIN TRANSACTION / ROLLBACK TRANSACTION block and add script that checks the results after the delete/update. I do this always, regardless if I have a backup or not"

# 2013/05/07/Adding-ASP-NET-SimpleMembership-to-an-existing-MVC-4-application/index.html

- filename: 2013/05/07/Adding-ASP-NET-SimpleMembership-to-an-existing-MVC-4-application/index.html
  article_title: Adding ASP.NET SimpleMembership to an existing MVC 4 application
  question: What template has membership functionality?
  answer: MVC 4
  context: Even though there is an MVC 4 template for a new Internet project that contains membership functionality, some people like to start building their apps from an empty project.

# 2013/05/27/Azure-Mobile-Services-tips-and-tricks/index.html

- filename: 2013/05/27/Azure-Mobile-Services-tips-and-tricks/index.html
  article_title: Azure Mobile Services tips and tricks
  question: Why doesn't Live SDK work on WP7?
  answer: it isn't suppose to work that way
  context: I'd like to start with Live SDK sample landing page for Windows Phone. There is a link to download it, and it says it's working on WP7. There is also a code sample. Great! Except, um, that's the code sample for WP8. Even though async and await are working on WP7, Live SDK isn't working with it. Why?! I was stuck until I found that it isn't suppose to work that way - you should use the event pattern instead.

# 2013/06/10/Delegates-in-action-part-2-ASP-NET-password-encryption/index.html

- filename: 2013/06/10/Delegates-in-action-part-2-ASP-NET-password-encryption/index.html
  article_title: "Delegates in action, part 2: ASP.NET password encryption"
  question: How do you set a password format in ASP.net?
  answer: set a single setting in your web.config
  context: Setting the password formats in ASP.Net is quite simple. All you need to do is to set a single setting in your web.config and you are ready to go. But what if you've already used Clear password format, your application is in a production environment, and you decide to change it to the Hashed password format? You cannot just change the PasswordFormat setting and expect that everything will work fine. You have to care of the old entries and convert them to the new password format.

- filename: 2013/06/10/Delegates-in-action-part-2-ASP-NET-password-encryption/index.html
  article_title: "Delegates in action, part 2: ASP.NET password encryption"
  question: What should you do before changing the password format?
  answer: make a database backup
  context: We strongly recommend you to make a database backup before you start playing either with password format conversion or some other data manipulation task.

# 2013/10/17/The-Geek-Gathering-2013/index.html

- filename: 2013/10/17/The-Geek-Gathering-2013/index.html
  article_title: The Geek Gathering 2013
  question: What is Osijek Software City?
  answer: association of local software companies and independent contractors
  context: The association of local software companies and independent contractors - known as Osijek Software City - now includes several hundred developers with different backgrounds and is growing stronger every month. Our goal is to put Osijek on a global IT industry map, and our members cover a large spectrum of different technologies, from Magento and LAMP stack, to Java and .NET.

# 2013/12/08/Async-upload-using-angular-file-upload-directive-and-net-WebAPI-service/index.html

- filename: 2013/12/08/Async-upload-using-angular-file-upload-directive-and-net-WebAPI-service/index.html
  article_title: Async upload using angular-file-upload directive and .NET WebAPI service
  question: What does angular-file-upload do?
  answer: handles file upload for you and lets you upload files asynchronously to the server
  context: Angular-file-upload directive is an awesome lightweight AngularJS directive which handles file upload for you and lets you upload files asynchronously to the server. This post will give you basic understanding on how to upload files by using this directive together with .NET WebAPI service.

# 2013/12/28/Managing-Azure-CORS-rules/index.html

- filename: 2013/12/28/Managing-Azure-CORS-rules/index.html
  article_title: Managing Azure CORS rules
  question: Why can you now access azure resources from any domain?
  answer: Microsoft recently introduced Cross-Origin Resource Sharing (CORS) support
  context: Microsoft recently introduced Cross-Origin Resource Sharing (CORS) support for Azure blobs, queues and tables.  This means you can now access Azure resources using AJAX-only calls from any domain which is quite handy. Imagine having a website hosted on your own server and allowing your users to upload large files to Azure Storage. Previously, you would need to upload and siphon all the data through your server and you would have to use Azure Storage SDK to do it. This means that processing time and bandwidth of your server are being wasted just for relaying data to its real destination. The good news is - you can now upload files to Azure Storage directly from the browser, using only client-side code and the Azure REST API, and without wasting additional server resources.

- filename: 2013/12/28/Managing-Azure-CORS-rules/index.html
  article_title: Managing Azure CORS rules
  question: How can you change Azure CORS rules?
  answer: using the Azure SDK
  context: "To be able to upload files this way, you would of course need to enable CORS for a particular Azure Storage account you want to use. There are two tips that could save you time with this:  as far as I know, you can't manage Azure CORS rules through the Azure management portal at this moment, it needs to be done using the Azure SDK"

# 2014/01/30/A-simple-and-nice-list-Sass-mixin-using-em-en-dashes/index.html

- filename: 2013/12/28/Managing-Azure-CORS-rules/index.html
  article_title: A simple and nice list Sass mixin using em-en dashes
  question: How can you make bulleted lists prettier?
  answer: using em dashes
  context: The default browser styles for the bulleted lists are a bit boring and not really pretty. Let's fix that up with a quick Sass mixin by using em dashes before the text.  Let's set up some basic variables first. The $list-nice-dash is an option between an en-dash and an em-dash respectively. Use $list-nice-generate-html-class variable for HTML class generation for the list (for further reuse).

# 2014/02/19/Building-Windows-Azure-Media-Services-async-CORS-enabled-upload/index.html

- filename: 2013/12/28/Managing-Azure-CORS-rules/index.html
  article_title: Building Windows Azure Media Services async CORS enabled upload
  question: What is a shared access signature?
  answer: an URL
  context: This brings us to the next point - when I say 'upload through WAMS', what I really mean by this is 'upload through a Shared Access Signature (SAS)'. SAS is basically an URL which we will create for each file (a video in our case) and it will be used to make consecutive AJAX upload requests against the Azure REST API. Each SAS has its validity duration and a permission type like read, write, list and delete. These will all be defined in the backend portion of the code. (if you want to know more about SAS, i recommend you read this great post about Azure SAS).

- filename: 2013/12/28/Managing-Azure-CORS-rules/index.html
  article_title: Building Windows Azure Media Services async CORS enabled upload
  question: What is a shared access signature used for?
  answer: make consecutive AJAX upload requests
  context: This brings us to the next point - when I say 'upload through WAMS', what I really mean by this is 'upload through a Shared Access Signature (SAS)'. SAS is basically an URL which we will create for each file (a video in our case) and it will be used to make consecutive AJAX upload requests against the Azure REST API. Each SAS has its validity duration and a permission type like read, write, list and delete. These will all be defined in the backend portion of the code. (if you want to know more about SAS, i recommend you read this great post about Azure SAS).

- filename: 2013/12/28/Managing-Azure-CORS-rules/index.html
  article_title: Building Windows Azure Media Services async CORS enabled upload
  question: What happens if you try to upload a file through a SAS URI without enabling CORS?
  answer: your upload PUT requests against Azure will be rejected
  context: To upload files through an SAS URI, Cross-Origin Resource Sharing needs to be enabled over at Azure for your particular domain. If you don't enable CORS, your upload PUT requests against Azure will be rejected. You can read a bit more about managing Azure CORS rules in my previous post. Normally you would have to set the rules up through your source-code, but since this is a one-time set-up, it makes no sense to keep this code in your project. That's why we made a simple web app for managing Azure CORS rules which you can download over at github. It will make it easier for you to add/edit the rules.

# 2014/05/05/Azure-compute-emulator-port-mapping-issue/index.html

# 2014/05/19/The-Geek-Gathering/index.html

- filename: 2014/05/19/The-Geek-Gathering/index.html
  article_title: The Geek Gathering
  question: What is the purpose of the geek gathering?
  answer: discovering new tricks and secrets about JavaScript, design, UX, cloud computing, databases, big data, Internet of things, Web application security, hybrid Web development and other cutting-edge software development topics.
  context: "If the name still doesn't sound familiar - The Geek Gathering is all about discovering new tricks and secrets about JavaScript, design, UX, cloud computing, databases, big data, Internet of things, Web application security, hybrid Web development and other cutting-edge software development topics. More details on its humble beginnings are available here. Mono was in charge of the content: we've been trying to bring world-class speakers to our part of the world, and according to the feedback, it appears that we did a good job."

- filename: 2014/05/19/The-Geek-Gathering/index.html
  article_title: The Geek Gathering
  question: Who opened the Geek Gathering in 2014?
  answer: Douglas Crockford
  context: The conference was opened by the man himself, Douglas Crockford. His talk, 'The Better Parts' was one of those out-of-the-box experiences. Many of us swear by his book, and were simultaneously surprised and inspired by his views on the new features in ECMAScript 6. I still don't have production-quality videos from this year's conference, so here's a good quality recording of the same talk he held at the neighboring conference just a month ago, and believe me, you don't want to miss it.

# 2014/06/20/Improving-AngularJS-long-list-rendering-performance-using-ReactJS/index.html

- filename: 2014/06/20/Improving-AngularJS-long-list-rendering-performance-using-ReactJS/index.html
  article_title: Improving AngularJS long list rendering performance using ReactJS
  question: What are the problems loading long lists in Angular?
  answer: slow rendering speed (always) huge amount of watchers
  context: "There are two problems with generating long lists in Angular:  slow rendering speed (always) huge amount of watchers (if your list contains multiple interactive elements or lots of dynamic data that changes over time)"

- filename: 2014/06/20/Improving-AngularJS-long-list-rendering-performance-using-ReactJS/index.html
  article_title: Improving AngularJS long list rendering performance using ReactJS
  question: What is the recommended number of watchers in Angular?
  answer: around 2000
  context: Slow rendering will make it longer for the data to actually show up in the browser (on initial and every subsequent render) and huge amount of watchers will make your website laggy and less responsive in general once everything is rendered. Keep in mind that recommended amount of watchers per page is around 2000. So if your list is not that long but 'only' has a lot of watchers, you might be able to work around the problem using bindonce which will significantly decrease the number of watchers on your page. If your list however is quite long, rendering it will take some time.

- filename: 2014/06/20/Improving-AngularJS-long-list-rendering-performance-using-ReactJS/index.html
  article_title: Improving AngularJS long list rendering performance using ReactJS
  question: What is JSX?
  answer: a Reacts compiler
  context: React-tools will provide you with JSX which is a Reacts compiler that transforms its JS/XML-like syntax into native JavaScript. To write ReactJS components you will need an empty JS file with the following line at the beginning of the file /** @jsx React.DOM */ and any ReactJS components code bellow that. After you run the JSX watcher (jsx --watch jsx_folder/ scripts_folder/) on folder where that JS file resides in it will look for changes and (re)create a second (native JS) file which you will need to include into your project.

# 2014/07/07/Creating-NodeJS-modules-with-both-promise-and-callback-API-support-using-Q/index.html

- filename: 2014/07/07/Creating-NodeJS-modules-with-both-promise-and-callback-API-support-using-Q/index.html
  article_title: Creating NodeJS modules with both promise and callback API support using Q
  question: What is Q?
  answer: one of the most popular and well rounded JS promise libraries with NodeJS support
  context: However, you might also want to support the error-first callback pattern as well, for all the developers out there who prefer to use the standard NodeJS approach. Since Q is currently one of the most popular and well rounded JS promise libraries with NodeJS support, in this post we'll be using Q to implement a module function that will support dual promise/callback API's.

# 2014/08/25/Sharing-sessions-between-SocketIO-and-Express-using-Redis/index.html

- filename: 2014/08/25/Sharing-sessions-between-SocketIO-and-Express-using-Redis/index.html
  article_title: Sharing sessions between SocketIO and Express using Redis
  question: What parts of an application is SocketIO best suited for?
  answer: real-time communication
  context: Even if you're using SocketIO, chances are you don't really need all your communication between the client and the server to go through sockets. For all the parts of your application that require real-time communication SocketIO will be perfect but for everything else you might be better off using standard REST API calls. Why? Without going into 'speed' and 'ease of implementation' pros and cons (there's plenty of that lying around the Internet) I see a few reasons why one might want to use both in the same application. REST is still a bit more 'standard' way to exchange data and if there is no real need to use websockets everywhere, your team might be a bit more comfortable using REST since it's what they've been using 'ever since'. Other than that, you might be in the middle of a project that previously used REST for everything and now the need arose to develop a component which requires real-time communication. Of course, you won't be rewriting all your code to SocketIO because there are more important things to dedicate your time to. Or you might simply believe SocketIO should be in charge of everything real-time and REST should be used to handle everything else - which is also perfectly fine. Either way, you'll most probably get into situation where you'll want to share session data between the two.

# 2015/01/08/Mono-a-redesigned-experience/index.html

- filename: 2015/01/08/Mono-a-redesigned-experience/index.html
  article_title: Redesigning the Mono experience
  question: What are mono's company values?
  answer: Great products and code Human centric design Quality as a tradition
  context: "The initial step for achieving better communication was redesigning the way we present Mono to the world. The time was right for redesigning our visual identity so it reflects our company values:  Great products and code Human centric design Quality as a tradition"

- filename: 2015/01/08/Mono-a-redesigned-experience/index.html
  article_title: Redesigning the Mono experience
  question: Why did mono change their visual identify from a pixel and blue color scheme?
  answer: too cold and generic
  context: We have done a lot of desk research and discussions on the logical evolutionary steps for our visual identity. The result was clear — we wanted to keep the core idea of a 'pixel' that was a cornerstone of our old visual identity. It is a good representation of the digital world, but we felt it did not represent our brand good enough on its own - however, we've decided to use it as a core element of the new design. The pixel, along with the blue color scheme was way too cold and generic, so that had to go away. To achieve a warmer, more human approach we changed our brand colors to vibrant red and elegant black.

# 2015/01/09/A-new-place-called-home/index.html

- filename: 2015/01/09/A-new-place-called-home/index.html
  article_title: A new place called home
  question: Why did mono make a new website?
  answer: add new value to our clients, users and the community
  context: Over the last couple of months we've been working hard on our new visual identity and the new Mono website. Today is the day where we share the results of this work with the rest of the world.  The idea behind the new site is to add new value to our clients, users and the community. Take your time to find out about our new products such as Baasic and Theor.io, or see what's been happening lately with MonoX and eCTD Office. You can meet the team and see what we've accomplished over the years.

# 2015/02/05/Importance-of-photography-in-web-design/index.html

- filename: 2015/02/05/Importance-of-photography-in-web-design/index.html
  article_title: Importance of photography in web design
  question: What does quality and reliability of a website design depend on?
  answer: how users percieve it
  context: Quality and reliability of the website design largely depends on how users percieve it. Good photography and visual design guides the users and communicates an atmosphere around a certain brand or product.

- filename: 2015/02/05/Importance-of-photography-in-web-design/index.html
  article_title: Importance of photography in web design
  question: What does good photography do for a brand?
  answer: guides the users and communicates an atmosphere
  context: Quality and reliability of the website design largely depends on how users percieve it. Good photography and visual design guides the users and communicates an atmosphere around a certain brand or product.

- filename: 2015/02/05/Importance-of-photography-in-web-design/index.html
  article_title: Importance of photography in web design
  question: Why should you hire a pro photographer?
  answer: Professional photographers are already familiar with lightning, they know their camera really well and don't lack in equipment that is too expensive for an amateur
  context: The best and most recommended option is to hire a pro. If you have a team member who is a semi-pro or an amateur photographer with a good set of lens and a DSLR you can take a walk on the wild side and make your own photos. Professional photographers are already familiar with lightning, they know their camera really well and don't lack in equipment that is too expensive for an amateur. On the other hand, hiring a professional does not guarantee quality of your photo story. Sometimes the story is not very well told by the photographer, sometimes the lightning/colours/tone in the photographs doesn't reflect your brand or product. Those things happen commonly so make sure you communicate your expectations and your brand with the professional down to the detail. Put pros and cons on paper and decide what is best for your project.

# 2015/03/24/Baasic-beta-now-opened/index.html

- filename: 2015/03/24/Baasic-beta-now-opened/index.html
  article_title: Baasic beta now opened
  question: What is Baasic?
  answer: Baasic provides a standard set of backend features
  context: "Here is a short introduction to this new product/service: Baasic provides a standard set of backend features - after all, BaaS in its name does not stand for nothing. For those of you not familiar with this term, please refer to our introductory article. However, this is only a beginning: Baasic hits a sweet spot on the intersection of today's BaaS solutions, content management systems and modern application frameworks. It offers end-to-end functionality for web and mobile application development that is not tied to a particular programming language and development framework."

- filename: 2015/03/24/Baasic-beta-now-opened/index.html
  article_title: Baasic beta now opened
  question: What is Baasic?
  answer: Baasic provides a standard set of backend features
  context: "Here is a short introduction to this new product/service: Baasic provides a standard set of backend features - after all, BaaS in its name does not stand for nothing. For those of you not familiar with this term, please refer to our introductory article. However, this is only a beginning: Baasic hits a sweet spot on the intersection of today's BaaS solutions, content management systems and modern application frameworks. It offers end-to-end functionality for web and mobile application development that is not tied to a particular programming language and development framework."

# 2015/04/23/Google-unwanted-software/index.html

- filename: 2015/04/23/Google-unwanted-software/index.html
  article_title: When Google says your software is unwanted
  question: What can cause your software to be classified as unwanted?
  answer: not having a link to the EULA along with uninstall instructions somewhere at the download page
  context: A short Google search revealed that there are not too many resources regarding this problem - I guess that this will change very soon. However, one post from the guys from HttpWatch was particularly informative and entertaining, and quickly got us into the right track. It appears that not having a link to the EULA along with uninstall instructions somewhere at the download page will immediately classify your software as 'unwanted'. We still think that it is important for a software company to have all terms and conditions displayed in plain sight, without loopholes, ambiguity or small print; however, there should be a gentler way to handle the issue of positioning links to such documents.

# 2015/09/04/Windows-Server-2012-Hyper-V-Cluster-ASP-NET-PostgreSQL-our-experiences-1/index.html

- filename: 2015/09/04/Windows-Server-2012-Hyper-V-Cluster-ASP-NET-PostgreSQL-our-experiences-1/index.html
  article_title: Windows Server 2012 clustering for high availability, with ASP.NET and PostgreSQL - Part 1
  question: Can postgresql scale horizontally?
  answer: it cannot scale out horizontally
  context: While we've used SQL Server in most of our previous implementations, after doing some preliminary tests, PostgreSQL proved to be a very capable relational database. After all, it has millions of implementations over its 30-year history. It is still being kept very relevant in the big data era by adding support for new data types, such as JSON - a feature that is essential for implementing dynamic types and similar functionality in Baasic and similar solutions. The capability to handle schema-less data simultaneously with traditional relational data is a big plus for us. However, as most of traditional RDBMSes designed to power transactional workloads, it cannot scale out horizontally (at least without specialized add-ons). When resources become thin, you would buy a bigger server, instead of scaling the load out to multiple smaller machines. While scale-out approach is becoming increasingly popular these days (mostly in various NoSQL incarnations), I would still argue that scaling up is a viable solution for a large class of problems. If you need to scale on a Google scale, scale out is the right way to go. However, we are not Google, and neither are most of the web sites and applications.

- filename: 2015/09/04/Windows-Server-2012-Hyper-V-Cluster-ASP-NET-PostgreSQL-our-experiences-1/index.html
  article_title: Windows Server 2012 clustering for high availability, with ASP.NET and PostgreSQL - Part 1
  question: What is the first rule of horizontally scaling a database?
  answer: avoid it at all costs
  context: To quote one of the posters at stackexchange, 'first rule of horizontal scaling of a database is to avoid it at all costs.'.  If you still need to scale out, there are now a few interesting solutions for SQL Server and Azure SQL Database. PostgreSQL has its own tools for the job, including CitusDB and Postgres-XL.

- filename: 2015/09/04/Windows-Server-2012-Hyper-V-Cluster-ASP-NET-PostgreSQL-our-experiences-1/index.html
  article_title: Windows Server 2012 clustering for high availability, with ASP.NET and PostgreSQL - Part 1
  question: What does STONITH stand for?
  answer: Shoot The Other Node In The Head
  context: In our case, we needed full support for failover - in a two DB servers scenarios, if the primary server fails then the standby server should begin failover procedures. On a side note, this is probably the right time to introduce concepts of cold, warm and hot standby nodes. So, when the old primary server restarts, we must have a mechanism for informing the old primary that it is no longer the primary. This is known as STONITH (Shoot The Other Node In The Head), and is essential to avoid situations where both systems think they are the primary, which is a sure recipe for disaster, split-brain scenario and, ultimately, data loss. On the other hand, attempts to use PostgreSQL in multi-master shared storage configurations result in extremely severe data corruption. To make things more interesting, PostgreSQL does not provide the tools required to identify a failure on the primary and notify the standby database server out of the box. We did some research and tested a lot of tools for solving this problem, and while some users are perfectly happy with their choices, we were somewhat reluctant. In addition to that, running Windows version of PostgreSQL in a production environment is really not the best idea - and we already had a set of Windows Server 2012 R2 Servers with Hyper-V role as a software infrastructure for a virtualized environment. Database servers are running Linux, as multiple flavors of it are supported for use as a guest operating system in a Hyper-V virtual machine.

- filename: 2015/09/04/Windows-Server-2012-Hyper-V-Cluster-ASP-NET-PostgreSQL-our-experiences-1/index.html
  article_title: Windows Server 2012 clustering for high availability, with ASP.NET and PostgreSQL - Part 1
  question: What can cause extreme data corruption in postgresql?
  answer: attempts to use PostgreSQL in multi-master shared storage configurations
  context: In our case, we needed full support for failover - in a two DB servers scenarios, if the primary server fails then the standby server should begin failover procedures. On a side note, this is probably the right time to introduce concepts of cold, warm and hot standby nodes. So, when the old primary server restarts, we must have a mechanism for informing the old primary that it is no longer the primary. This is known as STONITH (Shoot The Other Node In The Head), and is essential to avoid situations where both systems think they are the primary, which is a sure recipe for disaster, split-brain scenario and, ultimately, data loss. On the other hand, attempts to use PostgreSQL in multi-master shared storage configurations result in extremely severe data corruption. To make things more interesting, PostgreSQL does not provide the tools required to identify a failure on the primary and notify the standby database server out of the box. We did some research and tested a lot of tools for solving this problem, and while some users are perfectly happy with their choices, we were somewhat reluctant. In addition to that, running Windows version of PostgreSQL in a production environment is really not the best idea - and we already had a set of Windows Server 2012 R2 Servers with Hyper-V role as a software infrastructure for a virtualized environment. Database servers are running Linux, as multiple flavors of it are supported for use as a guest operating system in a Hyper-V virtual machine.

- filename: 2015/09/04/Windows-Server-2012-Hyper-V-Cluster-ASP-NET-PostgreSQL-our-experiences-1/index.html
  article_title: Windows Server 2012 clustering for high availability, with ASP.NET and PostgreSQL - Part 1
  question: What is a failover cluster?
  answer: a group of servers
  context: A failover cluster is a group of servers that work together to maintain high availability of applications and services (these are known as roles). If one of the servers (or nodes) fails, another node in the cluster can take over its workload without any downtime. In addition, the clustered roles are monitored to verify that they are working properly. If they are not working, they are restarted or moved to another node. Failover clusters also provide Cluster Shared Volume (CSV, more on that later) functionality that provides a consistent, distributed namespace that clustered roles can use to access shared storage from all nodes. Using this technology, you can scale up to 64 physical nodes and to 8,000 virtual machines.

- filename: 2015/09/04/Windows-Server-2012-Hyper-V-Cluster-ASP-NET-PostgreSQL-our-experiences-1/index.html
  article_title: Windows Server 2012 clustering for high availability, with ASP.NET and PostgreSQL - Part 1
  question: How many physical nodes can a failover cluster handle?
  answer: up to 64
  context: A failover cluster is a group of servers that work together to maintain high availability of applications and services (these are known as roles). If one of the servers (or nodes) fails, another node in the cluster can take over its workload without any downtime. In addition, the clustered roles are monitored to verify that they are working properly. If they are not working, they are restarted or moved to another node. Failover clusters also provide Cluster Shared Volume (CSV, more on that later) functionality that provides a consistent, distributed namespace that clustered roles can use to access shared storage from all nodes. Using this technology, you can scale up to 64 physical nodes and to 8,000 virtual machines.

- filename: 2015/09/04/Windows-Server-2012-Hyper-V-Cluster-ASP-NET-PostgreSQL-our-experiences-1/index.html
  article_title: Windows Server 2012 clustering for high availability, with ASP.NET and PostgreSQL - Part 1
  question: What is a virtual router?
  answer: an abstract representation of multiple routers
  context: "Just to briefly mention the rest of the network infrastructure (firewalls, routers and load balancers) in front of these servers. Everything is set up in a redundant fashion. Therefore, firewalls/routers are using Virtual Router Redundancy Protocol (VRRP) that introduce a concept of virtual routers, which are an abstract representation of multiple routers - master and backup - acting as a group. When an active routers fails, a backup router is automatically selected to replace it. In a similar fashion, we are using multiple HAProxy load balancers, along with multiple A DNS records for our service employing round robin technique. There are multiple approaches for achieving the full redundancy and avoiding single point of failure in this type of environment: for example, Stackoverflow apparently uses keepalived to ensure high availability; heartbeat is also an alternative. We opted for an alternative approach that combines DNS load balancing - which achieves a fairly rough balance of traffic between multiple load balancers and enable failover when one of load balancer dies - and using load balancers to do their job on a more granular level. 'DNS Load Balancing and Using Multiple Load Balancers in the Cloud' and 'How To Configure DNS Round-Robin Load-Balancing For High-Availability' describe 'our' approach in more details."

# 2015/10/07/Json-serialization-caching/index.html

- filename: 2015/10/07/Json-serialization-caching/index.html
  article_title: JSON serialization caching
  question: What is a fast json serializer?
  answer: the Json.NET serializer
  context: In this post, I will not introduce yet another super-fast serializer. I also promise not to rely on quick and dirty techniques to enhance the existing ones. I will introduce a pretty simple mechanism which will allow you to optimize Json serialization output. It is important to point out that the proposed solution will be made on top of the Json.NET serializer which is already one of the fastest serializers out there. The technique presented here should also be applicable to all other serializers.

- filename: 2015/10/07/Json-serialization-caching/index.html
  article_title: JSON serialization caching
  question: What causes the spikes in the serialization graph?
  answer: .Net's garbage collector
  context: On top of that, a graph has a few spikes, which look strange. After some time spent on the subject, we realized that the spikes are caused by the .Net's garbage collector which kicked in and caused a little serialization time discrepancy.  In the next two figures, you could see the performance results made on a 1000 and 10000 weather stations, with a 50000 and 500000 weather readings, respectively. 

# 2016/01/25/Hyper-V-Failover-Cluster-2/index.html

- filename: 2016/01/25/Hyper-V-Failover-Cluster-2/index.html
  article_title: "Windows Server 2012 Hyper-V Failover Clustering - Part 2: Prerequisites"
  question: How many servers run a Hyper-V hypervisor?
  answer: two
  context: The basic setup includes two servers running Hyper-V hypervisor and a Storage Area Network (SAN) as a shared storage mechanism. Additional Hyper-V servers can easily be added to achieve better scalability. In our scenario, each server contains 4 Gigabit network adapters (NICs) and one Fibre Channel adapter (or host bus adapter, HBA). This FC adapter is used to connect with the SAN.

- filename: 2016/01/25/Hyper-V-Failover-Cluster-2/index.html
  article_title: "Windows Server 2012 Hyper-V Failover Clustering - Part 2: Prerequisites"
  question: On the SAN, what does the DatabaseStorage volume store?
  answer: PostgreSQL databases
  context: "We are creating multiple volumes on our SAN that will serve different purposes: - SystemStorage volume will hold the system disk of our database server. - DatabaseStorage will be used to store PostgreSQL databases. - FileStorage will be exposed to the application servers as a central shared location where users can store and share files. - Witness will be used for failover cluster quorum configuration"

- filename: 2016/01/25/Hyper-V-Failover-Cluster-2/index.html
  article_title: "Windows Server 2012 Hyper-V Failover Clustering - Part 2: Prerequisites"
  question: What role needs to be added to the servers in Hyper-V clusters?
  answer: Hyper-V
  context: Basic installation and networking For a start, you will need to add the Hyper-V role to the servers. In Server Manager - Manage menu, click Add Roles and Features, select Role-based or feature-based installation, select the appropriate server, and pick Hyper-V on the Select server roles page. Leave all the default options on subsequent pages, and the Hyper-V role will get installed.

- filename: 2016/01/25/Hyper-V-Failover-Cluster-2/index.html
  article_title: "Windows Server 2012 Hyper-V Failover Clustering - Part 2: Prerequisites"
  question: How many NICs should be used in failover clustering?
  answer: at least two separate NICs
  context: "Is is always a good idea to use at least two separate NICs in failover clustering scenarios: the public interface is configured with the IP address that will be used to communicate with clients over the network, while the private interface is used for communicating with other cluster nodes ('heartbeat'). We will use an additional NIC for backup purposes - although this is not absolutely necessary, it will offload the traffic caused by regular backup tasks from the public interface. This leaves us with one free NIC, and we've decided to team two NICs on a main public interface. NIC team is basically a collection of network interfaces that work together as one, providing bandwidth aggregation and redundancy. It is easy to team NICs in Windows Server 2012, so I will not provide a step by step instructions on how to do that: here is an excellent article on NIC teaming that will guide you through the process. It is not a mandatory step for setting up a failover cluster."

# 2016/01/28/Hyper-V-Failover-Cluster-3/index.html

- filename: 2016/01/28/Hyper-V-Failover-Cluster-3/index.html
  article_title: "Windows Server 2012 Hyper-V Failover Clustering - Part 3: Installation"
  question: Is Failover Clustering available in the standard edition of Windows Server 2012?
  answer: Failover clustering is available in both Standard and Datacenter editions
  context: Failover clustering is available in both Standard and Datacenter editions of Windows Server 2012 R2. Generally speaking, clustering involves using two or more physical servers to create one 'logical' server that exposes some functionality to users or applications. The members of the cluster (called nodes) are able to monitor each other and, if one of them goes down, its functionality 'fails over' to other nodes without causing any disruption of service to the users.

- filename: 2016/01/28/Hyper-V-Failover-Cluster-3/index.html
  article_title: "Windows Server 2012 Hyper-V Failover Clustering - Part 3: Installation"
  question: Why do you need to validate your cluster environment after adding the failover clustering feature?
  answer: Microsoft does not provide support unless all the hardware passes all the tests
  context: "After the failover clustering feature is added, you need to validate the environment in which you will create a cluster. This is an essential step: Microsoft does not provide support unless all the hardware passes all the tests in the validation wizard. You can set up a cluster without validating the hardware, but chances are that you will experience problems sooner or later, so it does not make a lot of sense. So, go to the Failover Cluster Manager and click on the Validate Configuration link in the Management pane."

- filename: 2016/01/28/Hyper-V-Failover-Cluster-3/index.html
  article_title: "Windows Server 2012 Hyper-V Failover Clustering - Part 3: Installation"
  question: "What should you do if you get the error: No disks suitable for cluster disks were found"
  answer: make sure that your SAN is supported
  context: You can also add more disks at any time by clicking on the Add Disk option in the Actions pane of the Disks section. If you receive an error saying 'No disks suitable for cluster disks were found', make sure that your SAN is supported (validation process should report if it is not) and that your disks are initialized, visible on all nodes and have drive letter assigned to them. Alternatively, if you are using iSCSI targets, do not forget to configure the volumes at the iSCSI initiator on each node (Auto Configure button in the Volumes and Devices tab in the iSCSI Initiator Properties).

- filename: 2016/01/28/Hyper-V-Failover-Cluster-3/index.html
  article_title: "Windows Server 2012 Hyper-V Failover Clustering - Part 3: Installation"
  question: What feature in Windows Server makes shared disks concurrently accessible to all nodes in a failover cluster?
  answer: Cluster Shared Volumes
  context: "CSV (Cluster Shared Volumes) is a feature in Windows Server in which shared disks are concurrently accessible to all nodes within a failover cluster. You need to tell the cluster manager which storage should be used for the CSVs. In our scenario, we will use CSVs to hold disks for fault-tolerant virtual machines. As mentioned in the previous post, our standard arrangement involves something like this: - SystemStorage volume will hold the system disk of our database server. - DatabaseStorage will be used to store PostgreSQL databases. - FileStorage will be exposed to the application servers as a central shared location where users can store and share files. - Witness will be used for failover cluster quorum configuration."

# 2016/02/01/Hyper-V-Failover-Cluster-4-PostgreSQL/index.html

# 2016/02/18/SEO-for-javascript-applications/index.html

- filename: 2016/02/18/SEO-for-javascript-applications/index.html
  article_title: SEO for JavaScript applications, 2016 edition
  question: When did google start indexing content rendered by javascript?
  answer: early as 2008
  context: "Traditionally, search engine crawlers were only looking at the raw textual content contained within the HTTP response body and didn't really interpret what a typical browser running JavaScript would interpret. When pages that have a lot of content rendered by JavaScript started showing up, Google started crawling and indexing it as early as 2008, but in a rather limited fashion. Ajax crawling scheme was a standard, albeit clunky solution for this problem up until now. Google was putting a lot of work into a more elegant approach to understand web pages better, and finally, on Oct 14, 2015 they officially announced that the AJAX crawling scheme is now deprecated. In their own words:  We are no longer recommending the AJAX crawling proposal we made back in 2009."

- filename: 2016/02/18/SEO-for-javascript-applications/index.html
  article_title: SEO for JavaScript applications, 2016 edition
  question: What can you do to save a crawler from the effort of executing javascript on its own?
  answer: serve the prerendered HTML snapshot
  context: "letting you know that you should handle the response differently inside your JS application. On most occasions, you will serve the prerendered HTML snapshot to the crawler, saving it from the effort of parsing and executing JavaScript on its own.  The newer HTML5 pushState doesn't work the same way, as it modifies the browser's URL and history. If you are using pushState, you should use the following tag in the header of your pages:"

# 2016/02/22/Installing-Prerender-io/index.html

- filename: 2016/02/22/Installing-Prerender-io/index.html
  article_title: Installing Prerender.io
  question: What enables search engines to index dynamic content?
  answer: prerendering services
  context: As described in our previous article on SEO for JavaScript applications, prerendering services still play an important role in enabling search engine crawlers to index the dynamic content. This time we will learn how to install Prerender.io on your server infrastructure.

- filename: 2016/02/22/Installing-Prerender-io/index.html
  article_title: Installing Prerender.io
  question: What does prerender.io do?
  answer: optimize your JavaScript applications for search engines
  context: Prerender.io is a great service that allow you to optimize your JavaScript applications for search engines without an effort - you just purchase an account with them and install the middleware on your server. The source code for their service is available on GitHub and you can alternatively run it from your servers which is handy in high-volume scenarios. This is what we are using for our applications, and since the installation instructions are a bit terse, we’ve decided to document the whole process. We will be using a fresh and dedicated installation of Ubuntu 14.04 to run our local instance of Prerender.io.

- filename: 2016/02/22/Installing-Prerender-io/index.html
  article_title: Installing Prerender.io
  question: Why would you use the redis caching plugin?
  answer: it offers scalability and a simple cache expiration mechanism
  context: Generating HTML snapshots is a resource-intensive process, so some sort of caching strategy should be used to improve the performance. Prerender.io comes with several different caching plugins, but we will use the one based on Redis, as it offers scalability and a simple cache expiration mechanism. Redis can be built from source, but the pre-built version is sufficient for the task at hand.

# 2016/04/21/Ninject-ambient-scope-and-deterministic-dispose/index.html

- filename: 2016/04/21/Ninject-ambient-scope-and-deterministic-dispose/index.html
  article_title: Ninject ambient scope and deterministic dispose
  question: When would you define a custom lifetime or scope?
  answer: to reuse instances within the lifetime of a scope (ambient instances) and to specify a deterministic dispose of resources
  context: When working with Ninject, one can specify standard lifetime designators like InSingletonScope and InTransientScope in the bindings section. There are two important situations when one might want to define a custom scope - to reuse instances within the lifetime of a scope (ambient instances) and to specify a deterministic dispose of resources when a scope lifetime ends.

- filename: 2016/04/21/Ninject-ambient-scope-and-deterministic-dispose/index.html
  article_title: Ninject ambient scope and deterministic dispose
  question: What's the biggest challenge in making ninject aware of a custom scope?
  answer: propagating its context
  context: In order for the whole thing to work in the InNamedScope example above, BarFactory (or in a real life situation, a really complex chain of objects) needs to be injected by the parent scoping type (Foo or Goo or Moo) because that’s under Ninject’s control and that’s the only way it can pass the contextual parent scope information down to the injection chain. As soon as one moves away from the NamedScope extension way of doing things, the biggest challenge in accomplishing Ninject to be aware of a custom scope is not wiring it in the bindings, but actually propagating its context in an async/await environment.

- filename: 2016/04/21/Ninject-ambient-scope-and-deterministic-dispose/index.html
  article_title: Ninject ambient scope and deterministic dispose
  question: How can you deterministically dispose of an instance bound to a custom scope?
  answer: have that scope implement the Ninject’s INotifyWhenDisposed interface
  context: "Amazingly, googling Ninject deterministic dispose gives various confusing posts, so one may need to dig further to find a coherent answer. I found it nicely explained in this old article - Cache-and-Collect Lifecycle Management in Ninject 2.0. It seems that the only way to have a deterministic dispose of an instance bound to a custom scope is to have that scope implement the Ninject’s INotifyWhenDisposed interface. It will immediately cause the disposal of any IDisposable instance associated with the scope object when scope’s Dispose method is invoked. The key thing is to raise the INotifyWhenDisposed.Disposed event within the Dispose method. Here’s the code for the universal disposable scope:"

# 2016/07/13/Building-highly-available-applications-with-Amazon-RDS/index.html

- filename: 2016/07/13/Building-highly-available-applications-with-Amazon-RDS/index.html
  article_title: Building highly available applications with Amazon RDS
  question: What does AWS do?
  answer: (AWS) provides a platform that is well suited for building fault-tolerant software
  context: Amazon Web Services (AWS) provides a platform that is well suited for building fault-tolerant software. Similar services are offered by other cloud providers, and most of them provide the infrastructure needed for building fault-tolerant systems that operate with a minimal amount of human interaction and up-front financial investment. We have chosen AWS because of its sheer size, popularity, and price structure. More importantly, we were looking to avoid complicated and time-consuming administrative tasks such as database installation and upgrades; storage management; replication for high availability and read throughput; and backups for disaster recovery. Over the past few years, PostgreSQL has become the preferred open source relational database for many developers, and we are using it in most of our new applications. Amazon Relational Database Service (RDS) makes it easy to set up, operate, and scale PostgreSQL databases in the cloud, and provides support for Amazon Aurora, MySQL, MariaDB, Oracle and SQL Server too. It is an ideal choice for both smaller shops without an experienced DBA, and larger organizations that need to scale their solutions.

- filename: 2016/07/13/Building-highly-available-applications-with-Amazon-RDS/index.html
  article_title: Building highly available applications with Amazon RDS
  question: What let's you use AWS resources in a virtual network?
  answer: Amazon Virtual Private Cloud
  context: Amazon Virtual Private Cloud (VPC) lets you provision a logically isolated section of the “big' Amazon cloud where you can launch AWS resources in a virtual network that you define. Since our database instance only needs to be available to the web server - and not to the public Internet - we create a VPC with both public and private subnets. The web server will be hosted in the public subnet. The database instance will be hosted in a private subnet. The web server will be able to connect to the database instance because it is hosted within the same VPC, but the database instance will not be available to the public Internet, providing greater security.

- filename: 2016/07/13/Building-highly-available-applications-with-Amazon-RDS/index.html
  article_title: Building highly available applications with Amazon RDS
  question: What causes Amazon RDS to automatically perform a failover?
  answer: an infrastructure failure
  context: A word about Multi-AZ deployments, which turns out to be one of the coolest database features we use in the AWS. When you provision a Multi-AZ DB Instance, Amazon automatically creates a primary DB Instance and synchronously replicates the data to a standby instance in a different Availability Zone. Each AZ has its own physically distinct infrastructure. In the case of an infrastructure failure, Amazon RDS performs an automatic failover to the standby, so that you can resume database operations as soon as the failover is complete - in our initial tests, this takes less than a minute. Since the endpoint for your DB Instance remains the same after a failover, your application can resume database operation without the need for any manual intervention.

- filename: 2016/07/13/Building-highly-available-applications-with-Amazon-RDS/index.html
  article_title: Building highly available applications with Amazon RDS
  question: What web service supports the redis in-memory caching engine?
  answer: Amazon ElastiCache
  context: Amazon ElastiCache is a web service that makes it easy to deploy, operate, and scale an in-memory cache. It supports two open-source in-memory caching engines, Memcached and Redis. We are using Redis in our projects, and ElastiCache supports Master / Slave replication and Multi-AZ, which can be used to achieve cross zone redundancy.

# 2016/08/03/Please-Move-Here-Comes-Angular2/index.html

- filename: 2016/08/03/Please-Move-Here-Comes-Angular2/index.html
  article_title: Please move, here comes Angular 2
  question: What does typescript compile to?
  answer: plain JavaScript
  context: If you are using AngularJS as your UI framework, be prepared, Angular 2 delivers a substantial change. No more plain old JavaScript - say hi to Typescript. Well, Angular 2 can be written in JavaScript, Typescript, and Dart, but sympathy mostly goes to Typescript as it is the language Angular 2 was written in. Although it does eventually compile to plain JavaScript, Typescript is a powerful language that lets you build things that would otherwise take time and time to do. What you're actually doing with Angular 2 is writing templates in Typescript. Angular 2 then turns that templates into code that's highly optimized for today's JavaScript virtual machines, says the Angular 2 team, but more on that later.

- filename: 2016/08/03/Please-Move-Here-Comes-Angular2/index.html
  article_title: Please move, here comes Angular 2
  question: What are the small chunks of code called which later are connected to form the application?
  answer: components
  context: Angular 2 is entirely component based; there are no $scopes or controllers. Instead, you are writing small chunks of code that are later connected to form one big application. Those small chunks of code are components - below you can see an example of one. The component is defined by the @Component decorator, and each one has its own custom selector (HTML tag), template (or template URL for external template), directives, providers, and pipes (which we'll talk about later). Each component also has its own class called component class. In order to use (import) it somewhere else, we have to export it.

- filename: 2016/08/03/Please-Move-Here-Comes-Angular2/index.html
  article_title: Please move, here comes Angular 2
  question: What do all Angular component names end with?
  answer: Component
  context: As for the naming convention, each component name ends with Component, and the name of the file that holds it with .component. All the files are in kebab-case, so no worries about case sensitivity. Applied to the example component above, the file name would look like this: my-custom.component.ts.

- filename: 2016/08/03/Please-Move-Here-Comes-Angular2/index.html
  article_title: Please move, here comes Angular 2
  question: What are attribute directives?
  answer: the ones that change the appearance or behavior of an element they are applied to
  context: Attribute directives are the ones that change the appearance or behavior of an element they are applied to. These directives are specified by their selector using square brackets (read more about attribute directives here).  Structural directives are here to add, remove, and replace elements in DOM. If you used AngularJS, you have seen them or used them plenty of times. Like in AngularJS, ng-if and ng-repeat exist in Angular 2 too, but are slightly changed. Structural directives now start with a “*”(*ngFor, *ngIf). If you see that in an HTML code, that is a structural directive (read more about structural directives here).

- filename: 2016/08/03/Please-Move-Here-Comes-Angular2/index.html
  article_title: Please move, here comes Angular 2
  question: In Angular 2, what is the name of the root component?
  answer: AppComponent
  context: What is also new is that there is no bootstrap directive. What we are doing instead is calling the bootstrap function and passing in the root application component. By convention, root component is called “AppComponent”.

- filename: 2016/08/03/Please-Move-Here-Comes-Angular2/index.html
  article_title: Please move, here comes Angular 2
  question: What's the difference between promises and observables?
  answer: Observables are lazy
  context: In Angular 2, services are often used to fetch the data from the server. While doing so they use HTTP client that is built in Angular 2. There was a slight twist in how the data can be fetched. Just as AngularJS use promises, Angular 2 uses promises, but wait… Angular 2 new favorite way of getting data is via Observables! Now, observables are great. They are a part of Reactive Extensions. If you haven’t seen them yet, it’s time to learn more. But what’s the difference between promises and observables? Observables are lazy - meaning you have to subscribe to them in order to trigger their behavior and you can also cancel your request thanks to disposability. Ben Lesh compared them in this great video which is worth watching.

# 2016/11/28/WebCamp-2016/index.html
- filename: 2016/11/28/WebCamp-2016/index.html
  article_title: 
  question: What is Baasic?
  answer: Baasic provides a standard set of backend features
  context: Here is a short introduction to this new product/service: Baasic provides a standard set of backend features - after all, BaaS in its name does not stand for nothing. For those of you not familiar with this term, please refer to our introductory article. However, this is only a beginning: Baasic hits a sweet spot on the intersection of today’s BaaS solutions, content management systems and modern application frameworks. It offers end-to-end functionality for web and mobile application development that is not tied to a particular programming language and development framework. 
